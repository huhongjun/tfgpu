{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. tensorflow-gpu测试代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "hello = tf.constant('Hello, Word!')  \n",
    "sess = tf.Session()  \n",
    "print(sess.run(hello))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpu+gpu运行\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    a = tf.constant([1.0,2.0,3.0],shape=[3],name='a')\n",
    "    b = tf.constant([1.0,2.0,3.0],shape=[3],name='b')\n",
    "with tf.device('/gpu:0'):\n",
    "    c = a+b\n",
    "   \n",
    "#注意：allow_soft_placement=True表明：计算设备可自行选择，如果没有这个参数，会报错。\n",
    "#因为不是所有的操作都可以被放在GPU上，如果强行将无法放在GPU上的操作指定到GPU上，将会报错。\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=True))\n",
    "#sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(sess.run(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linux下指定GPU\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=0 python gpu.py gpu 1500\n",
    "\n",
    "# !CUDA_VISIBLE_DEVICES=0 python tensorflow/examples/tutorials/mnist/mnist_with_summaries.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 性能测试\n",
    "\n",
    "可以使用CPU或GPU分别计算，比较性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254430800000.0\n",
      "\n",
      "\n",
      "\n",
      "Shape: (10000, 10000) Device: /cpu:0\n",
      "Time taken: 0:00:19.444104\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "# 设置为gpu或cpu分别执行\n",
    "device_name=\"/cpu:0\" \n",
    " \n",
    "shape=(int(10000),int(10000))\n",
    " \n",
    "with tf.device(device_name):\n",
    "    #形状为shap,元素服从minval和maxval之间的均匀分布\n",
    "    random_matrix = tf.random_uniform(shape=shape, minval=0, maxval=1)\n",
    "    dot_operation = tf.matmul(random_matrix, tf.transpose(random_matrix))\n",
    "    sum_operation = tf.reduce_sum(dot_operation)\n",
    " \n",
    "startTime = datetime.now()\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as session:\n",
    "    result = session.run(sum_operation)\n",
    "print(result)\n",
    " \n",
    "print(\"\\n\" * 2)\n",
    "print(\"Shape:\", shape, \"Device:\", device_name)\n",
    "print(\"Time taken:\", datetime.now() - startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看GPU内存使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Driver: b'418.39'\n",
      "Memory total:4040M, used:558M, free:3481M\n",
      "Device 0 : b'GeForce GTX 1050 Ti'\n",
      "Temperature is 55 C\n",
      "Power ststus 0\n"
     ]
    }
   ],
   "source": [
    "import pynvml\n",
    "\n",
    "pynvml.nvmlInit()\n",
    "\n",
    "# 显示驱动信息\n",
    "print(\"GPU Driver: %s\" % pynvml.nvmlSystemGetDriverVersion())\n",
    "\n",
    "# 查询GPU内存信息\n",
    "handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "meminfo = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "print('Memory total:%sM, used:%sM, free:%sM' % (int(meminfo.total/1024**2), int(meminfo.used/1024**2), int(meminfo.free/1024**2))) \n",
    "\n",
    "# 查询GPU显卡型号\n",
    "deviceCount = pynvml.nvmlDeviceGetCount()#几块显卡\n",
    "for i in range(deviceCount):\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "\n",
    "    print(\"Device\", i, \":\", pynvml.nvmlDeviceGetName(handle)) \n",
    "    print(\"Temperature is %d C\"%pynvml.nvmlDeviceGetTemperature(handle,0))\n",
    "    print(\"Power ststus\",pynvml.nvmlDeviceGetPowerState(handle))\n",
    "    \n",
    "pynvml.nvmlShutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G7-7588 显卡信息\n",
    "\n",
    "\n",
    "GeForce GTX 1050 Ti \n",
    "Pascal 768 Up to 4 GB GDDR5\n",
    "\n",
    "---------------------\n",
    "GeForce GTX 1050\n",
    "Pascal 640Up to 4 GB GDDR5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_gpu_available(cuda_only=True):\n",
    "  \"\"\"\n",
    "  code from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/test.py\n",
    "  Returns whether TensorFlow can access a GPU.\n",
    "  Args:\n",
    "    cuda_only: limit the search to CUDA gpus.\n",
    "  Returns:\n",
    "    True iff a gpu device of the requested kind is available.\n",
    "  \"\"\"\n",
    "  from tensorflow.python.client import device_lib as _device_lib\n",
    "\n",
    "  if cuda_only:\n",
    "    return any((x.device_type == 'GPU')\n",
    "               for x in _device_lib.list_local_devices())\n",
    "  else:\n",
    "    return any((x.device_type == 'GPU' or x.device_type == 'SYCL')\n",
    "               for x in _device_lib.list_local_devices())\n",
    "\n",
    "def get_available_gpus():\n",
    "    \"\"\"\n",
    "    code from http://stackoverflow.com/questions/38559755/how-to-get-current-available-gpus-in-tensorflow\n",
    "    \"\"\"\n",
    "    from tensorflow.python.client import device_lib as _device_lib\n",
    "    local_device_protos = _device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "def test_is_gpu_available():\n",
    "    #for i in range(4):\n",
    "    if(is_gpu_available()):\n",
    "        str = 'with' if is_gpu_available(True) else 'without'\n",
    "        print(\"GPU is available, %s CUDA installed\" % str)\n",
    "\n",
    "def test_get_available_gpus():\n",
    "    devices = get_available_gpus();\n",
    "    i = 0 \n",
    "    for d in devices:\n",
    "        i = i + 1\n",
    "        print(\"你的第 %2d 个GPU是： %5s\" % (i,d))\n",
    "        \n",
    "test_is_gpu_available()\n",
    "\n",
    "test_get_available_gpus()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "    测试NVIDIA cuda。\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "value = np.random.randn(5000,1000)\n",
    "a = tf.constant(value)\n",
    "\n",
    "b = a*a\n",
    "\n",
    "tic = time.time()\n",
    "with tf.Session() as sess:\n",
    "    for i in range(1000):\n",
    "        sess.run(b)\n",
    "        \n",
    "toc = time.time()\n",
    "t_cost = toc - tic\n",
    "\n",
    "print(t_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = tf.constant([1.0, 2.0, 3.0], shape=[3], name='a')\n",
    "b = tf.constant([1.0, 2.0, 3.0], shape=[3], name='b')\n",
    "c = a + b\n",
    "# 通过log_device_placement参数来输出运行每一个运算的设备。\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "print(sess.run(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)) as sess:\n",
    "    a = tf.constant(1)\n",
    "    b = tf.constant(3)\n",
    "    c = a + b\n",
    "    print('结果是：%d\\n 值为：%d' % (sess.run(c), sess.run(c)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python gpu.py gpu 1500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment Variable Syntax      Results\n",
    "\n",
    "    CUDA_VISIBLE_DEVICES=1           Only device 1 will be seen\n",
    "    CUDA_VISIBLE_DEVICES=0,1         Devices 0 and 1 will be visible\n",
    "    CUDA_VISIBLE_DEVICES=\"0,1\"       Same as above, quotation marks are optional\n",
    "    CUDA_VISIBLE_DEVICES=0,2,3       Devices 0, 2, 3 will be visible; device 1 is masked\n",
    "    CUDA_VISIBLE_DEVICES=\"\"          No GPU will be visible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定使用的GPU\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.7)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "#device_name = sys.argv[1]  # Choose device from cmd line. Options: gpu or cpu\n",
    "device_name = 'gpu'\n",
    "#shape = (int(sys.argv[2]), int(sys.argv[2]))\n",
    "shape = (1500,1500)\n",
    "if device_name == \"gpu\":\n",
    "    device_name = \"/gpu:0\"\n",
    "else:\n",
    "    device_name = \"/cpu:0\"\n",
    "\n",
    "with tf.device(device_name):\n",
    "    random_matrix = tf.random_uniform(shape=shape, minval=0, maxval=1)\n",
    "    dot_operation = tf.matmul(random_matrix, tf.transpose(random_matrix))\n",
    "    sum_operation = tf.reduce_sum(dot_operation)\n",
    "\n",
    "\n",
    "startTime = datetime.now()\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as session:\n",
    "        result = session.run(sum_operation)\n",
    "        print(result)\n",
    "\n",
    "# It can be hard to see the results on the terminal with lots of output -- add some newlines to improve readability.\n",
    "print(\"\\n\" * 5)\n",
    "print(\"Shape:\", shape, \"Device:\", device_name)\n",
    "print(\"Time taken:\", datetime.now() - startTime)\n",
    "\n",
    "print(\"\\n\" * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from timeit import default_timer as timer\n",
    "from numba import vectorize\n",
    "\n",
    "@vectorize([\"float32(float32, float32)\"], target='cuda')\n",
    "def vectorAdd(a, b):\n",
    "    return a + b\n",
    "\n",
    "def main():\n",
    "    N = 320000000\n",
    "\n",
    "    A = np.ones(N, dtype=np.float32 )\n",
    "    B = np.ones(N, dtype=np.float32 )\n",
    "    C = np.zeros(N, dtype=np.float32 )\n",
    "\n",
    "    start = timer()\n",
    "    C = vectorAdd(A, B)\n",
    "    vectorAdd_time = timer() - start\n",
    "\n",
    "    print(\"c[:5] = \" + str(C[:5]))\n",
    "    print(\"c[-5:] = \" + str(C[-5:]))\n",
    "\n",
    "    print(\"vectorAdd took %f seconds \" % vectorAdd_time)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!lspci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ubuntu\n",
    "\n",
    "lspci|grep - nvidia 即可看到当前显卡型号\n",
    "nvidia-settings nvidia-prime双显卡切换\n",
    "nvidia-settings 弹出nvidia信息窗口，可以看到驱动版本，GPU编号等\n",
    "sudo dpkg --list | grep nvidia-*\n",
    "\n",
    "NVIDIA Driver Version: 384.130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dpkg --list | grep nvidia-*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ubuntu查看nvidia显卡状态\n",
    "nvidia-smi\n",
    "\n",
    "连续查看显卡状态\n",
    "sudo watch -n 10 nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一栏的Fan：N/A是风扇转速，从0到100%之间变动，这个速度是计算机期望的风扇转速，实际情况下如果风扇堵转，可能打不到显示的转速。有的设备不会返回转速，因为它不依赖风扇冷却而是通过其他外设保持低温（比如我们实验室的服务器是常年放在空调房间里的）。 \n",
    "\n",
    "第二栏的Temp：是温度，单位摄氏度。 \n",
    "第三栏的Perf：是性能状态，从P0到P12，P0表示最大性能，P12表示状态最小性能。 \n",
    "第四栏下方的Pwr：是能耗，上方的Persistence-M：是持续模式的状态，持续模式虽然耗能大，但是在新的GPU应用启动时，花费的时间更少，这里显示的是off的状态。 \n",
    "第五栏的Bus-Id是涉及GPU总线的东西，domain:bus:device.function \n",
    "第六栏的Disp.A是Display Active，表示GPU的显示是否初始化。 \n",
    "第五第六栏下方的Memory Usage是显存使用率。 \n",
    "第七栏是浮动的GPU利用率。 \n",
    "第八栏上方是关于ECC的东西。 \n",
    "第八栏下方Compute M是计算模式。 \n",
    "下面一张表示每个进程占用的显存使用率。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}