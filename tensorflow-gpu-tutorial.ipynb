{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. tensorflow-gpu测试代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "hello = tf.constant('Hello, Word!')  \n",
    "sess = tf.Session()  \n",
    "print(sess.run(hello))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpu+gpu运行\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    a = tf.constant([1.0,2.0,3.0],shape=[3],name='a')\n",
    "    b = tf.constant([1.0,2.0,3.0],shape=[3],name='b')\n",
    "with tf.device('/gpu:0'):\n",
    "    c = a+b\n",
    "   \n",
    "#注意：allow_soft_placement=True表明：计算设备可自行选择，如果没有这个参数，会报错。\n",
    "#因为不是所有的操作都可以被放在GPU上，如果强行将无法放在GPU上的操作指定到GPU上，将会报错。\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=True))\n",
    "#sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(sess.run(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python gpu.py gpu 1500\n",
    "# !CUDA_VISIBLE_DEVICES=0 python tensorflow/examples/tutorials/mnist/mnist_with_summaries.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看GPU内存使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GeForce GTX 1050 Ti \n",
    "Pascal 768 Up to 4 GB GDDR5\n",
    "\n",
    "---------------------\n",
    "GeForce GTX 1050\n",
    "Pascal 640Up to 4 GB GDDR5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_gpu_available(cuda_only=True):\n",
    "  \"\"\"\n",
    "  code from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/test.py\n",
    "  Returns whether TensorFlow can access a GPU.\n",
    "  Args:\n",
    "    cuda_only: limit the search to CUDA gpus.\n",
    "  Returns:\n",
    "    True iff a gpu device of the requested kind is available.\n",
    "  \"\"\"\n",
    "  from tensorflow.python.client import device_lib as _device_lib\n",
    "\n",
    "  if cuda_only:\n",
    "    return any((x.device_type == 'GPU')\n",
    "               for x in _device_lib.list_local_devices())\n",
    "  else:\n",
    "    return any((x.device_type == 'GPU' or x.device_type == 'SYCL')\n",
    "               for x in _device_lib.list_local_devices())\n",
    "\n",
    "def get_available_gpus():\n",
    "    \"\"\"\n",
    "    code from http://stackoverflow.com/questions/38559755/how-to-get-current-available-gpus-in-tensorflow\n",
    "    \"\"\"\n",
    "    from tensorflow.python.client import device_lib as _device_lib\n",
    "    local_device_protos = _device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "def test_is_gpu_available():\n",
    "    #for i in range(4):\n",
    "    if(is_gpu_available()):\n",
    "        str = 'with' if is_gpu_available(True) else 'without'\n",
    "        print(\"GPU is available, %s CUDA installed\" % str)\n",
    "\n",
    "def test_get_available_gpus():\n",
    "    devices = get_available_gpus();\n",
    "    i = 0 \n",
    "    for d in devices:\n",
    "        i = i + 1\n",
    "        print(\"你的第 %2d 个GPU是： %5s\" % (i,d))\n",
    "        \n",
    "test_is_gpu_available()\n",
    "\n",
    "test_get_available_gpus()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "    测试NVIDIA cuda。\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "value = np.random.randn(5000,1000)\n",
    "a = tf.constant(value)\n",
    "\n",
    "b = a*a\n",
    "\n",
    "tic = time.time()\n",
    "with tf.Session() as sess:\n",
    "    for i in range(1000):\n",
    "        sess.run(b)\n",
    "        \n",
    "toc = time.time()\n",
    "t_cost = toc - tic\n",
    "\n",
    "print(t_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = tf.constant([1.0, 2.0, 3.0], shape=[3], name='a')\n",
    "b = tf.constant([1.0, 2.0, 3.0], shape=[3], name='b')\n",
    "c = a + b\n",
    "# 通过log_device_placement参数来输出运行每一个运算的设备。\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "print(sess.run(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)) as sess:\n",
    "    a = tf.constant(1)\n",
    "    b = tf.constant(3)\n",
    "    c = a + b\n",
    "    print('结果是：%d\\n 值为：%d' % (sess.run(c), sess.run(c)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python gpu.py gpu 1500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment Variable Syntax      Results\n",
    "\n",
    "    CUDA_VISIBLE_DEVICES=1           Only device 1 will be seen\n",
    "    CUDA_VISIBLE_DEVICES=0,1         Devices 0 and 1 will be visible\n",
    "    CUDA_VISIBLE_DEVICES=\"0,1\"       Same as above, quotation marks are optional\n",
    "    CUDA_VISIBLE_DEVICES=0,2,3       Devices 0, 2, 3 will be visible; device 1 is masked\n",
    "    CUDA_VISIBLE_DEVICES=\"\"          No GPU will be visible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定使用的GPU\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.7)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "#device_name = sys.argv[1]  # Choose device from cmd line. Options: gpu or cpu\n",
    "device_name = 'gpu'\n",
    "#shape = (int(sys.argv[2]), int(sys.argv[2]))\n",
    "shape = (1500,1500)\n",
    "if device_name == \"gpu\":\n",
    "    device_name = \"/gpu:0\"\n",
    "else:\n",
    "    device_name = \"/cpu:0\"\n",
    "\n",
    "with tf.device(device_name):\n",
    "    random_matrix = tf.random_uniform(shape=shape, minval=0, maxval=1)\n",
    "    dot_operation = tf.matmul(random_matrix, tf.transpose(random_matrix))\n",
    "    sum_operation = tf.reduce_sum(dot_operation)\n",
    "\n",
    "\n",
    "startTime = datetime.now()\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as session:\n",
    "        result = session.run(sum_operation)\n",
    "        print(result)\n",
    "\n",
    "# It can be hard to see the results on the terminal with lots of output -- add some newlines to improve readability.\n",
    "print(\"\\n\" * 5)\n",
    "print(\"Shape:\", shape, \"Device:\", device_name)\n",
    "print(\"Time taken:\", datetime.now() - startTime)\n",
    "\n",
    "print(\"\\n\" * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from timeit import default_timer as timer\n",
    "from numba import vectorize\n",
    "\n",
    "@vectorize([\"float32(float32, float32)\"], target='cuda')\n",
    "def vectorAdd(a, b):\n",
    "    return a + b\n",
    "\n",
    "def main():\n",
    "    N = 320000000\n",
    "\n",
    "    A = np.ones(N, dtype=np.float32 )\n",
    "    B = np.ones(N, dtype=np.float32 )\n",
    "    C = np.zeros(N, dtype=np.float32 )\n",
    "\n",
    "    start = timer()\n",
    "    C = vectorAdd(A, B)\n",
    "    vectorAdd_time = timer() - start\n",
    "\n",
    "    print(\"c[:5] = \" + str(C[:5]))\n",
    "    print(\"c[-5:] = \" + str(C[-5:]))\n",
    "\n",
    "    print(\"vectorAdd took %f seconds \" % vectorAdd_time)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:00.0 Host bridge: Intel Corporation Device 3e10 (rev 07)\r\n",
      "00:01.0 PCI bridge: Intel Corporation Sky Lake PCIe Controller (x16) (rev 07)\r\n",
      "00:02.0 VGA compatible controller: Intel Corporation Device 3e9b\r\n",
      "00:04.0 Signal processing controller: Intel Corporation Skylake Processor Thermal Subsystem (rev 07)\r\n",
      "00:08.0 System peripheral: Intel Corporation Sky Lake Gaussian Mixture Model\r\n",
      "00:12.0 Signal processing controller: Intel Corporation Device a379 (rev 10)\r\n",
      "00:14.0 USB controller: Intel Corporation Device a36d (rev 10)\r\n",
      "00:14.2 RAM memory: Intel Corporation Device a36f (rev 10)\r\n",
      "00:14.3 Network controller: Intel Corporation Device a370 (rev 10)\r\n",
      "00:15.0 Serial bus controller [0c80]: Intel Corporation Device a368 (rev 10)\r\n",
      "00:15.1 Serial bus controller [0c80]: Intel Corporation Device a369 (rev 10)\r\n",
      "00:16.0 Communication controller: Intel Corporation Device a360 (rev 10)\r\n",
      "00:17.0 SATA controller: Intel Corporation Device a353 (rev 10)\r\n",
      "00:1b.0 PCI bridge: Intel Corporation Device a32c (rev f0)\r\n",
      "00:1d.0 PCI bridge: Intel Corporation Device a330 (rev f0)\r\n",
      "00:1d.5 PCI bridge: Intel Corporation Device a335 (rev f0)\r\n",
      "00:1f.0 ISA bridge: Intel Corporation Device a30d (rev 10)\r\n",
      "00:1f.3 Audio device: Intel Corporation Device a348 (rev 10)\r\n",
      "00:1f.4 SMBus: Intel Corporation Device a323 (rev 10)\r\n",
      "00:1f.5 Serial bus controller [0c80]: Intel Corporation Device a324 (rev 10)\r\n",
      "01:00.0 3D controller: NVIDIA Corporation Device 1c8c (rev a1)\r\n",
      "3b:00.0 Non-Volatile memory controller: Toshiba America Info Systems Device 0113 (rev 01)\r\n",
      "3c:00.0 Ethernet controller: Qualcomm Atheros Killer E2400 Gigabit Ethernet Controller (rev 10)\r\n"
     ]
    }
   ],
   "source": [
    "!lspci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ubuntu\n",
    "\n",
    "lspci|grep - nvidia 即可看到当前显卡型号\n",
    "nvidia-settings nvidia-prime双显卡切换\n",
    "nvidia-settings 弹出nvidia信息窗口，可以看到驱动版本，GPU编号等\n",
    "sudo dpkg --list | grep nvidia-*\n",
    "\n",
    "NVIDIA Driver Version: 384.130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ii  nvidia-384                                  384.130-0ubuntu0.16.04.1                     amd64        NVIDIA binary driver - version 384.130\r\n",
      "ii  nvidia-opencl-icd-384                       384.130-0ubuntu0.16.04.1                     amd64        NVIDIA OpenCL ICD\r\n",
      "ii  nvidia-prime                                0.8.2                                        amd64        Tools to enable NVIDIA's Prime\r\n",
      "ii  nvidia-settings                             361.42-0ubuntu1                              amd64        Tool for configuring the NVIDIA graphics driver\r\n"
     ]
    }
   ],
   "source": [
    "!dpkg --list | grep nvidia-*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ubuntu查看nvidia显卡状态\n",
    "nvidia-smi\n",
    "\n",
    "连续查看显卡状态\n",
    "sudo watch -n 10 nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一栏的Fan：N/A是风扇转速，从0到100%之间变动，这个速度是计算机期望的风扇转速，实际情况下如果风扇堵转，可能打不到显示的转速。有的设备不会返回转速，因为它不依赖风扇冷却而是通过其他外设保持低温（比如我们实验室的服务器是常年放在空调房间里的）。 \n",
    "\n",
    "第二栏的Temp：是温度，单位摄氏度。 \n",
    "第三栏的Perf：是性能状态，从P0到P12，P0表示最大性能，P12表示状态最小性能。 \n",
    "第四栏下方的Pwr：是能耗，上方的Persistence-M：是持续模式的状态，持续模式虽然耗能大，但是在新的GPU应用启动时，花费的时间更少，这里显示的是off的状态。 \n",
    "第五栏的Bus-Id是涉及GPU总线的东西，domain:bus:device.function \n",
    "第六栏的Disp.A是Display Active，表示GPU的显示是否初始化。 \n",
    "第五第六栏下方的Memory Usage是显存使用率。 \n",
    "第七栏是浮动的GPU利用率。 \n",
    "第八栏上方是关于ECC的东西。 \n",
    "第八栏下方Compute M是计算模式。 \n",
    "下面一张表示每个进程占用的显存使用率。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
